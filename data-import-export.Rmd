# 数据搬运工 {#data-porter}

导入数据与导出数据，各种数据格式，数据库

## 读写数据集 {#read-write-data}

```{r,echo=TRUE}
apropos("^read.")
apropos("^write.")
```

### `scan` {#scan}

### `read.csv` 与 `write.csv` {#read-write-csv}

`read.csv2` 与 `write.csv2` 

### `read.table` 与 `write.table` {#read-write-table}

变量名是不允许以下划线开头的，同样在数据框里，列名也不推荐使用下划线开头。默认情况下，`read.table` 都会通过参数 `check.names` 检查列名的有效性，该参数实际调用了函数 `make.names` 去检查。如果想尽量保持数据集原来的样子可以设置参数 `check.names = FALSE, stringsAsFactors = FALSE`。 默认情形下，`read.table` 还会将字符串转化为因子变量，这是 R 的历史原因，作为一门统计学家的必备语言，在统计模型中，字符常用来描述类别，而类别变量在 R 环境中常用因子类型来表示，而且大量内置的统计模型也是将它们视为因子变量，如 lm 、glm 等

```{r,echo=TRUE}
dat1 = read.table(header = TRUE, check.names = TRUE, text = "
_a _b _c
1 2 a1
3 4 a2
")
dat1
dat2 = read.table(header = TRUE, check.names = FALSE, text = "
_a _b _c
1 2 a1
3 4 a2
")
dat2
dat3 = read.table(header = TRUE, check.names = FALSE, stringsAsFactors = FALSE, text = "
_a _b _c
1 2 a1
3 4 a2
")
dat3
```

### `readLines` 与 `writeLines` {#read-write-lines}

让我们折腾一波，读进来又写出去，只有 R 3.5.3 以上才能保持原样的正确输入输出

```{r,echo=TRUE,eval=FALSE}
writeLines(readLines("latex/TeXLive.pkgs"),"latex/TeXLive.pkgs")
```



### `readRDS` 与 `saveRDS` {#read-save-rds}

序列化数据操作，Mark Klik 开发的 [fst](https://github.com/fstpackage/fst) 和 [Travers Ching](https://travers.im/) 开发的 [qs](https://github.com/traversc/qs)

[feather](https://github.com/wesm/feather/tree/master/R) 跨语言环境快速的读写数据

Table: (\#tab:fst-vs-others) fst 序列化数据框对象性能比较 BaseR、 data.table 和 feather [^fst-performance]

| Method         | Format  | Time (ms) | Size (MB) | Speed (MB/s) | N       |
| :------------- | :------ | :-------- | :-------- | :----------- | :------ |
| readRDS        | bin     | 1577      | 1000      | 633          | 112     |
| saveRDS        | bin     | 2042      | 1000      | 489          | 112     |
| fread          | csv     | 2925      | 1038      | 410          | 232     |
| fwrite         | csv     | 2790      | 1038      | 358          | 241     |
| read\_feather  | bin     | 3950      | 813       | 253          | 112     |
| write\_feather | bin     | 1820      | 813       | 549          | 112     |
| **read\_fst**  | **bin** | **457**   | **303**   | **2184**     | **282** |
| **write\_fst** | **bin** | **314**   | **303**   | **3180**     | **291** |

目前比较好的是 qs 和 fst 包 


[^fst-performance]: https://www.fstpackage.org/ 

### `read.ftable` 与 `write.ftable` {#read-write-ftable}


### `readBin` 与 `writeBin` {#read-write-bin}


### `readChar` 与 `writeChar` {#read-write-char}


### `read.dcf` 与 `write.dcf` {#read-write-dcf}


### `readClipboard` 与 `writeClipboard` {#read-write-clipboard}


### `read.socket` 与 `write.socket` {#read-write-socket}

### `load` 与 `save` {#load-save}



导入数据，也叫读入数据，读取数据，读入到 R 环境中，从磁盘上的文件，变成存储在R环境中的数据对象

写入压缩格式文件


`capture.output` 参数为表达式

`with` 与 `attach`
`sink`


```r
sink("sink-examp.txt")
i <- 1:10
outer(i, i, "*")
sink()
```

只将 `outer` 的结果保存到 `sink-examp.txt` 文件



### `source` 与 `dump` {#source-dump}

`dump` 保存数据对象 iris 到文件 `iris.txt`，文件内容是 R 命令，可把`iris.txt`看作代码文档执行，dput 保存数据对象内容到文件`iris.dat`，文件中不包含变量名 iris。注意到 `dump` 输入是一个字符串，而 `dput` 要求输入数据对象的名称，`source` 函数与 `dump` 对应，而 `dget` 与 `dput`对应。 

```{r,eval=FALSE,echo=TRUE}
data(iris)
dump('iris', file = 'iris.txt') 
source(file = 'iris.txt')
```

### `dget` 与 `dput` {#dget-dput}

```{r,eval=FALSE,echo=TRUE}
dput(iris, file = 'iris.dat')
iris <- dget(file = 'iris.dat')
```


读取 SPSS Matlab SAS 等软件生成的数据文件

戏称 [rio](https://github.com/leeper/rio) 包是数据导入导出 Input/Output  的瑞士军刀

### 其它数据格式 {#other-data-source}

jsonlite 与 yaml

jsonlite 包读取 `*.json` 格式的文件，`jsonlite::write_json` 函数将 R对象保存为 JSON 文件，`jsonlite::fromJSON` 将 json 字符串或文件转化为 R 对象，`jsonlite::toJSON` 函数正好与之相反

```{r,echo=TRUE,eval=FALSE}
library(jsonlite)
jsonlite::read_json(path = "path/to/filename.json")
```

yaml 包读取 `*.yml` 格式文件，返回一个列表，`yaml::write_yaml` 函数将 R 对象写入 yaml 格式 

```{r,echo=TRUE}
library(yaml)
yaml::read_yaml(file = '_bookdown.yml')
```

yaml 和 json 格式互转

## 批量读写 {#batch-import-export}

```{r,echo=TRUE,eval=FALSE}
read_list <- function(list_of_datasets, read_func) {
  read_and_assign <- function(dataset, read_func) {
    dataset_name <- as.name(dataset)
    dataset_name <- read_func(dataset)
  }

  # invisible is used to suppress the unneeded output
  output <- invisible(
    sapply(list_of_datasets,
      read_and_assign,
      read_func = read_func, simplify = FALSE, USE.NAMES = TRUE
    )
  )

  # Remove the extension at the end of the data set names
  names_of_datasets <- c(unlist(strsplit(list_of_datasets, "[.]"))[c(T, F)])
  names(output) <- names_of_datasets
  return(output)
}

data_files <- list.files(pattern = ".csv") # 提取文件名.csv格式文件
# 文件列表
print(data_files)

library("readr")
library("tibble")
# 读取文件
list_of_data_sets <- read_list(data_files, read_csv)
# 查看数据
glimpse(list_of_data_sets)
```


## 读写数据库 {#read-write-database}

将大量的 txt 文本存进 MySQL 数据库中，通过操作数据库来聚合文本，极大降低内存消耗 [^txt-to-mysql]

[Hands-On Programming with R](https://rstudio-education.github.io/hopr) 数据读写章节[^dataio]

[^dataio]: https://rstudio-education.github.io/hopr/dataio.html

ODBC 与 DBI 是其它数据库接口的基础

制作一个表格，左边数据库右边 R 接口，都包含链接，如表 \@ref(tab:dbi) 所示

```{r dbi}
db2r <- data.frame(
  db = c("MySQL", "SQLite", "PostgreSQL", "MariaDB"),
  db_urls = c(
    "https://www.mysql.com/",
    "https://www.sqlite.org",
    "https://www.postgresql.org/",
    "https://mariadb.org/"),
  dbi = c("RMySQL", "RSQLite", "RPostgres", "RMariaDB"),
  dbi_urls = c(
    "https://github.com/r-dbi/RMySQL",
    "https://github.com/r-dbi/RSQLite",
    "https://github.com/r-dbi/RPostgres",
    "https://github.com/r-dbi/RMariaDB"
  )
)
# db <-  paste0("[", db2r$db, "](", db2r$db_urls, ")")
# dbi <- paste0("[", db2r$dbi, "](", db2r$dbi_urls, ")")
knitr::kable(db2r, col.names = c("数据库","官网","R接口","开发仓"), caption = "数据库接口")
```

### RSQLite

参考 [bookdown-SQL](https://bookdown.org/yihui/rmarkdown/language-engines.html#sql)

```{r,echo=TRUE,cache=FALSE}
library(DBI)
con <- dbConnect(RSQLite::SQLite(), ":memory:")
dbWriteTable(con, "mtcars", mtcars, overwrite=TRUE)
subjects = 6
```

`max.print=10` 控制显示的行数

```{sql connection="con",tab.cap = "My Caption",max.print=10,cache=FALSE}
SELECT * FROM mtcars where gear IN (3, 4) and cyl >= ?subjects
```

`output.var="mtcars34"` 将查询的结果保存到变量 `mtcars34`，此外，还支持 SQL 语句中包含变量 subjects

```{sql connection="con",tab.cap = "My Another Caption", output.var="mtcars34"}
SELECT * FROM mtcars where gear IN (3, 4) and cyl >= ?subjects
```

```{r,echo=TRUE}
head(mtcars34)
```
```{r,include=FALSE}
dbDisconnect(conn = con)
```

### PostgreSQL

[odbc](https://github.com/r-dbi/odbc) 可以支持很多数据库，下面以连接 [PostgreSQL](https://www.postgresql.org/) 数据库为例介绍其过程

在 MacOS 上安装

```bash
# 安装 unixODBC 库
brew install unixodbc
# 安装 PostgreSQL
brew install psqlodbc
```

在 Ubuntu 上配置

```bash
# Install the unixODBC library
sudo apt-get install unixodbc unixodbc-dev
# PostgreSQL ODBC ODBC Drivers
sudo apt-get install odbc-postgresql
```

建立连接

```{r,echo=TRUE,eval=FALSE}
library(DBI)
con <- dbConnect(odbc::odbc(),
  driver = "PostgreSQL Driver",
  database = "test_db",
  uid = "postgres",
  pwd = "password",
  host = "localhost",
  port = 5432)
# host 可以是云端的地址也可以是本地局域网IP地址，
# 例如"192.168.1.200"这样的。
# 列出数据库中的所有表
dbListTables(con)
```

### ClickHouse

对系统的要求是 System requirements: Linux, x86_64 with SSE 4.2.

```{bash,eval=FALSE,echo=TRUE}
sudo apt-get install dirmngr    # optional
sudo apt-key adv --keyserver keyserver.ubuntu.com --recv E0C56BD4    # optional

echo "deb http://repo.yandex.ru/clickhouse/deb/stable/ main/" | sudo tee /etc/apt/sources.list.d/clickhouse.list
sudo apt-get update

sudo apt-get install -y clickhouse-server clickhouse-client
# 启动服务
sudo service clickhouse-server start
# 进入客户端
clickhouse-client
```

[ClickHouse 中文用户手册](https://clickhouse.yandex/docs/zh/) 和新浪高鹏在北京 [ClickHouse社区分享会](https://clickhouse.yandex/blog/en/clickhouse-community-meetup-in-beijing-on-january-27-2018) 上给的报告 [MySQL DBA解锁数据分析的新姿势-ClickHouse](https://yandex.github.io/clickhouse-presentations/meetup12/power_your_data.pdf) 

开源社区对 ClickHouse 提供了很多语言的支持，比如 R 语言接口有 [RClickhouse](https://github.com/IMSMWU/RClickhouse) 和 [clickhouse-r](https://github.com/hannesmuehleisen/clickhouse-r)，其它接口请看 [官方文档链接](https://clickhouse.yandex/docs/zh/interfaces/third-party/client_libraries/)

由于 clickhouse-r 还在开发中，从未提交到 CRAN，提供的功能也相对有限，这里我们推荐使用 RClickhouse，首先安装 RClickhouse

```{r,eval=FALSE,echo=TRUE}
# 安装 CRAN 版本
install.packages("RClickhouse")
# 安装Github上的开发版
devtools::install_github("IMSMWU/RClickhouse")
```

建立数据库的连接，离不开 `dbConnect` 函数，它提供的 `config_paths` 参数用来指定配置文件 `RClickhouse.yaml` 的路径，在该文件中指定一系列的参数值 `host, port, db, user, password, compression`

```yaml
host: example-db.com
port: 1111
```

如果没有手动配置，会使用默认的参数配置 `host="localhost", port = 9000, db = "default", user = "default", password = "", compression = "lz4"`

RClickHouse 提供部分 dplyr 式的数据操作

```{r,eval=identical(.Platform$OS.type,'unix') & !is_on_travis,echo=TRUE}
library(RClickhouse)
# 查询数据
library(dplyr, warn.conflicts = FALSE)

# 建立数据库连接
# con <- DBI::dbConnect(RClickhouse::clickhouse(), config_paths = "~/.R/RClickhouse.yaml")
con <- DBI::dbConnect(RClickhouse::clickhouse())

# 往 ClickHouse 中写入数据
# DBI::dbWriteTable(con, "mtcars", mtcars, overwrite=TRUE)

# 列出 ClickHouse 中存放的表
dbListTables(con)
# 列出表 mtcars 中的所有字段
dbListFields(con, "mtcars")
# 按变量 cyl 分组对 mpg 求和
tbl(con, "mtcars") %>% 
  group_by(cyl) %>% 
  summarise(smpg=sum(mpg, na.rm = TRUE)) # SQL 总是要移除缺失值

# 等价于
aggregate(mpg ~ cyl, data = mtcars, sum)

# 先筛选出 cyl = 8 并且 vs = 0 的数据，然后按 am 分组，最后对 qsec 求平均值
tbl(con, "mtcars") %>% 
  filter(cyl == 8, vs == 0) %>% 
  group_by(am) %>% 
  summarise(mean(qsec, na.rm = TRUE))

# 等价于
aggregate(qsec ~ am, data = mtcars, mean, subset = cyl == 8 & vs == 0)

# 不使用数据库的时候一定记得关闭数据库的连接
dbDisconnect(con)
```

::: sidebar
aggregate 聚合函数默认对缺失值的处理是忽略， sum 和 mean 函数的参数 `na.rm=TRUE` 实际由聚合函数 aggregate 的参数 `na.action` 传递，它的默认值是 `na.omit` ，就是将缺失值移除后返回。值得注意的是 `na.omit` 是一个缺失值处理的函数，所以如果对缺失值有特殊要求，比如插补，可以自己写函数传递给 `na.action` 参数
:::

你当然可以继续使用 SQL 语句做查询，而不使用 dplyr 提供的现代化的管道操作语法

```{r,eval=identical(.Platform$OS.type,'unix') & !is_on_travis,echo=TRUE}
# 建立数据库连接
con <- DBI::dbConnect(RClickhouse::clickhouse())
# 传递 SQL 查询语句
DBI::dbGetQuery(con, 
"SELECT
    vs,
    COUNT(*) AS n_vc,
    AVG(qsec) AS avg_qsec
FROM mtcars
GROUP BY vs")
# 不使用数据库的时候一定记得关闭数据库的连接
dbDisconnect(con)
```

::: sidebar
如果数据集比较小，可以将 ClickHouse 的整张表读进内存，但是对于大数据集，只有使用远程服务器才可以获得更好的性能

```{r,echo=TRUE,eval=FALSE}
# 读取数据库中的整张表
mtcars <- dbReadTable(con, mtcars)
```
:::

还有 RClickhouse 使用 SQL 查询的时候，同样支持 ClickHouse 的内置函数，如 `multiIf`

```{r,eval=identical(.Platform$OS.type,'unix') & !is_on_travis,echo=TRUE}
# 建立数据库连接
con <- DBI::dbConnect(RClickhouse::clickhouse())
# 查看 ClickHouse 中所有的数据库名称
DBI::dbGetQuery(con, "SHOW DATABASES")
# 查看所有存储的表
DBI::dbGetQuery(con, "SHOW TABLES")
# 获取 ClickHouse 中 mtcars 表的变量名和类型描述
DBI::dbGetQuery(con, "DESCRIBE TABLE mtcars")
# Compact CASE - WHEN - THEN conditionals
DBI::dbGetQuery(con, "
SELECT multiIf(am=1, 'automatic', 'manual') AS transmission,
       multiIf(vs=1, 'straight', 'V-shaped') AS engine
FROM mtcars
")

# 不使用数据库的时候一定记得关闭数据库的连接
dbDisconnect(con)
```

这是一个存放在 Github 上的包，随着 ClikHouse 在大厂的流行，此包也受到越来越多的关注
与数据仓库如何连接，如何查询数据，背后的接口 DBI 如何使用，实例化一个新的接口，如 clickhouse2r

[ClickHouse](https://clickhouse.yandex/) 独辟蹊径，基于 C++ 的实现，数据查询速度超级快，官网介绍碾压大量传统数据库。还有不少接口，其中还有 R 的 [clickhouse-r](https://github.com/hannesmuehleisen/clickhouse-r)

```{r,eval=FALSE,echo=TRUE}
# 安装
devtools::install_github("hannesmuehleisen/clickhouse-r")
# 调用接口
library(DBI)
con <- dbConnect(clickhouse::clickhouse(),
  host = "localhost",
  port = 8123L,
  user = "default",
  password = ""
)
dbWriteTable(con, "mtcars", mtcars)
dbListTables(con)
dbGetQuery(con, "SELECT COUNT(*) FROM mtcars")
d <- dbReadTable(con, "mtcars")
dbDisconnect(con)
```

发现它和 knitr  里的 SQL 钩子，都用 [DBI包](https://github.com/rstats-db/DBI)

```{r clickhouse,fig.cap="ClickHouse 与 R"}
knitr::include_graphics(path = "figures/clickhouse.png")
```

### MySQL

MySQL 是一个很常见，应用也很广泛的数据库，数据分析的常见环境是在一个R Notebook 里，我们可以在正文之前先设定数据库连接信息

````markdown
`r ''````{r setup}
library(DBI)
# 指定数据库连接信息
db <- dbConnect(RMySQL::MySQL(),
  dbname = "dbtest",
  username = "user_test",
  password = "password",
  host = "10.10.101.10",
  port = 3306
)
# 创建默认连接
knitr::opts_chunk$set(connection = "db")
# 设置字符编码，以免中文查询乱码
DBI::dbSendQuery(db, "SET NAMES utf8")
# 设置日期变量，以运用在SQL中
idate <- "2019-05-03"
```
````

SQL 代码块中使用 R 环境中的变量，并将查询结果输出为R环境中的数据框

````markdown
`r ''````{sql, output.var="data_output"}
SELECT * FROM user_table where date_format(created_date,'%Y-%m-%d')>=?idate  
```
````

以上代码会将 SQL 的运行结果存在 `data_output` 这是数据库中，idate 取之前设置的日期`2019-05-03`，`user_table` 是 MySQL 数据库中的表名，`created_date` 是创建`user_table`时，指定的日期名。

如果 SQL 比较长，为了代码美观，把带有变量的 SQL 保存为`demo.sql`脚本，只需要在 SQL 的 chunk 中直接读取 SQL 文件[^sql-chunck]。

````markdown
`r ''````{sql, code=readLines("demo.sql"), output.var="data_output"}
```
````

如果我们需要每天或者按照指定的日期重复地运行这个 R Markdown 文件，可以在 YAML 部分引入参数[^params-knit]

````markdown
---
params:
  date: "2019-05-03"  # 参数化日期
---

`r ''````{r, setup, include=FALSE}
idate = params$date # 将参数化日期传递给 idate 变量
```
````

我们将这个 Rmd 文件命名为 `MyDocument.Rmd`，运行这个文件可以从 R 控制台执行或在 RStudio 点击 knit。

```{r,echo=TRUE,eval=FALSE}
rmarkdown::render("MyDocument.Rmd", params = list(
  date = "2019-05-03"
))
```

如果在文档的 YAML 位置已经指定日期，这里可以不指定。注意在这里设置日期会覆盖 YAML 处指定的参数值，这样做的好处是可以批量化操作。

## Spark

当数据分析报告遇上 Spark 时，就需要 SparkR、 sparklyr 或 rsparking 接口了[^the-r-spark]


[^the-r-spark]: https://therinspark.com/
[^sql-chunck]: https://d.cosx.org/d/419974
[^txt-to-mysql]: https://brucezhaor.github.io/blog/2016/08/04/batch-process-txt-to-mysql
[^params-knit]: https://bookdown.org/yihui/rmarkdown/params-knit.html
